# Requirements

Required packages can be found in [the requirements file](requirements.txt).

# How to run

The main script is `v2_interview.py`, which can be run as follows:
```
    python v2_interview.py
```
and will by default simulate all 20 [classification](#classification) questions for all 50 users.

## Options

For an overview over all options, see
```
    python v2_interview.py --help
```

## General Options

### Number of simulated users

By default, 50 users will be simulated for all questions (20 for classification, 5 for agreement). This can be changed using the `--number_users` option (maximum: 50).

There are also different options for how humans are sampled from the original dataset. By default, they are sampled randomly. To choose a different option for this, use the `--select_users` option with one of the following values:
- `random`
- `first`
- `last`

#### Example
To simulate the first 5 humans of the dataset:
```
    python v2_interview.py --number_users=5 --select_users=first
```

### General profiling

Additionally, the following profiling prompt will be prepended as a [system prompt](https://platform.openai.com/docs/guides/prompt-engineering/tactic-ask-the-model-to-adopt-a-persona) by default:

> You are AGE years old, your gender is GENDER and your employment status is best described as EMPLOYMENT. With machine learning models and intelligent agents, you have [some/no] experience as a user and [some/no] experience as a developer. You are confronted with questions for a user study. Give answers in tune with your personality and previous answers if there are any.

The placeholders/options will be adapted to match the specific human that is simulated.

To exclude this system prompt, use the `--without_profiling` option. To explicitly include it, there is also a `--with_profiling` option.

#### Example
```
python v2_interview.py --without_profiling
```

## Classification

The full default prompt, passed when no additional options are given, is as follows:

> **USER**:
> "Attached is an image that consists of two sub-images. The top of the given image displays the original image of a bird and the bottom displays the same image combined with a heatmap that was generated by an explainable artificial intelligence model to explain the species predicted for the image.\
> You think the model distinguishes the four possible species classes based on the following features if they are highlighted by the heatmap:\
> \- Rhinoceros Auklets: HEATMAP_FEATURES_RA\
> \- Least Auklets: HEATMAP_FEATURES_LA\
> \- Parakeet Auklets: HEATMAP_FEATURES_PA\
> \- Crested Auklets: HEATMAP_FEATURES_CA\
> Based on the descriptions and the areas highlighted by the heatmap, which bird species do you think was predicted for the given image? Choose one of the following options for your answer:\
> \- Crested Auklet\
> \- Least Auklet\
> \- Parakeet Auklet\
> \- Rhinoceros Auklet\
> Answer with the bird name only.

where the `HEATMAP_FEATURES` will be replaced by human descriptions of example heatmaps for each bird species.

### Prompt options

#### Profiling

The `--without_profiling` option described in the [general options](#general-options) adapts the prompting to include example images for each bird species instead of human descriptions:

> **USER**
> Attached is an image that consists of two sub-images. The top of the given image displays the original image of a bird and the bottom displays the same image combined with a heatmap that was generated by an explainable artificial intelligence model to explain the species predicted for the image. As guidance, here are some example images that can help you understand how each bird type is distinguished:
>
> Crested Auklet: [EXAMPLE]\
> Least Auklet: [EXAMPLE]\
> Parakeet Auklet: [EXAMPLE]\
> Rhinoceros Auklet: [EXAMPLE]
> 
> Now, please consider the following image. Based on the examples and the areas highlighted by the heatmap, which bird species do you think was predicted for the given image? Choose one of the following options for your answer:
> 
> Crested Auklet\
> Least Auklet\
> Parakeet Auklet\
> Rhinoceros Auklet\
>
> Answer with the bird name only.

#### Prompting/Reasoning options

To select a prompt variation, use the `--reasoning` option. The available options are described in the following.

##### No reasoning
`none`: without reasoning (default)

##### Profile first

`profile_first`: Instead of 
> Answer with the bird name only

the following instruction will be given:
> First, describe all areas of the bird that are highlighted in the heatmap. Finally, for each bird description given, explain why it might or might not be that species of bird. Conclude your answer by stating only the selected option in the last line of your answer.

##### Heatmap first

`heatmap_first`: For this version, pre-generated (LLM) heatmap descriptions are inserted before the section in which the human descriptions are listed.

##### Gold heatmap first

`gold_heatmap_first`: This is a variant of `heatmap_first`, in which manually written gold standard heatmap descriptions are used instead of LLM-generated ones.

##### Chain of thought

`chain_of_thought`: In this version, an example reasoning is provided before asking the intended question.

### Question selection

For the classification questions, there are two ways to choose which questions will be simulated.

#### Automatic

`auto` automatically chooses a number of questions (can be chosen with `--number_questions`, default: 20) in one of three ways (can be chosen with `--select_questions`, default: `balanced`):
    - `random`
    - `balanced` (keeps the number of questions similar between classes)
    - `first`

For example, to simulate the first 5 questions, run
```
    python v2_interview.py auto --number_questions=5 --select_questions=first
```

#### Manual

`manual` allows you to choose individiual questions by their number. At least one number must be given with the `--questions` option.

For example, to simulate questions 1, 7, and 13, run
```
    python v2_interview.py manual --questions 1 7 13
```

## Agreement

To simulate the agreement statements/questions, run the following command:
```
    python v2_interview.py agreement
```

*NOTE: to use the [general options](#general-options) for this, place them **before** the `agreement` part of the command.*

By default, the prompt sent to the LLM will look like this:

> Previously, you were given images of birds. Each image was combined with a heatmap that was generated by an explainable artificial intelligence (XAI) model to explain the species predicted for the image by a classification model. For each of the images, you had to guess which of four bird species was predicted for it based on the heatmap that was generated for the image. You believed that the classification model distinguishes between the four possible species classes based on the following features that need to be highlighted by the heatmap: \
> \- Rhinoceros Auklets: HEATMAP_FEATURES_RA\
> \- Least Auklets: HEATMAP_FEATURES_LA\
> \- Parakeet Auklets: HEATMAP_FEATURES_PA\
> \- Crested Auklets: HEATMAP_FEATURES_CA\
> Out of 20 images you were confronted with, you guessed the classification correctly for 12 of them. Now you are asked to evaluate your XAI user study experience.\
> Rate your level of agreement for the following question: [EXAMPLE_QUESTION]. Answer on a scale of 1 to 7, where 1 means completely disagree and 7 completely agree. Answer with the number only.\
> [EXAMPLE_ANSWER]\
> Rate your level of agreement for the following question: [QUESTION]. Answer on a scale of 1 to 7, where 1 means completely disagree and 7 completely agree. Answer with the number only.


There are multiple additional options for this.

#### Questions to simulate
- `--questions`: which of the agreement questions to simulate (by default: all questions, if `--with_example` is set, without the example question)

#### Prompting

##### Examples
`--example`: which question to show the LLM as an example (by default: the first one)

`--with_example` or `--without_example` : whether to include a question as example (by default: with example) - note that if `--without_example` is set, any arguments to the previous `--example` option will be ignored

##### Accuracy
- `--with_accuracy` or `--without_accuracy`: to set whether to include the number of questions that the user answered correctly in the prompt (by default: with accuracy)

##### Average
- `--with_average` or `--without_average`: whether to include the average human score for the agreement questions as profiling (by default: without average)
- `--fixed_average` or `--user_average`: whether to include the average human score for the agreement questions as profiling or use a fixed value (4) (by default: user average)

Example:
```
    python v2_interview.py agreement --without_accuracy --questions 2 3 4
```

*NOTE: the questions that are actually simulated are computed as the questions in the `--questions` argument minus the `--example` question.*

# Outputs

All generated outputs can be found in the [out](out/) folder.
There is one subfolder with results from the agreement question simulations containing separate files for simulations with/without accuracy:
```
> agreement
```
For the bird questions, there are multiple subfolders:
```
> no_reason
> reason_chain_of_thought
> reason_heatmap_first
> reason_gold_heatmap_first
> reason_profile_first
```
In each subfolder, there are two more subfolders corresponding to the profiling options:
```
> no_profile
> profile
```
containing both a protocol and a results file.

The protocol includes the full prompts sent to the LLM and its answers as raw text, while the results file is a CSV file containing only the answers to each question.